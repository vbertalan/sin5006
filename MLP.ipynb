{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'serie2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a699d53e89a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;31m# load and prepare data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'serie2.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m \u001b[0mtreinamento\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;31m#scaler = MinMaxScaler(feature_range=(-1, 1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;31m#dataset = scaler.fit_transform(treinamento)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-a699d53e89a1>\u001b[0m in \u001b[0;36mload_csv\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mcsv_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_reader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'serie2.csv'"
     ]
    }
   ],
   "source": [
    "# Backprop on the Seeds Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            floats = [float(x) for x in row]\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(floats)\n",
    "    return dataset\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "    return stats\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        #for i in range(len(row)-1):\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    #return 1.0 / (1.0 + exp(-activation))\n",
    "    return np.tanh(activation)\n",
    "\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    #return output * (1.0 - output)\n",
    "    return 1.0 - np.tanh(output)**2\n",
    "\n",
    "# Forward propagate feito para lidar com quaisquer camadas ocultas, camada de saída é linear\n",
    "def forward_propagate3(network, row):\n",
    "    inputs = row\n",
    "    for layer in network[:-1]:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    output_layer = network[-1]\n",
    "    new_outputs = []\n",
    "    for output_neuron in output_layer:\n",
    "        activation = activate(output_neuron['weights'], inputs)\n",
    "        output_neuron['output'] = activation\n",
    "        new_outputs.append(output_neuron['output'])\n",
    "    outputs = new_outputs\n",
    "    return outputs\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                error = expected[j] - neuron['output']\n",
    "                errors.append(error)\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "# Treinamento com expected setado com base em valores float\n",
    "def train_network2(network, dataset, l_rate, n_epoch, n_outputs, stop_rate):\n",
    "    expected = list()\n",
    "    sum_error_old = 0.000\n",
    "    for i in range(len(dataset)-1):\n",
    "        expected.append(dataset[i+1][-1])\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for i in range(len(dataset)-1):\n",
    "            valor = dataset[i][0]\n",
    "            outputs = forward_propagate3(network, dataset[i])\n",
    "            expected_value = expected[i]\n",
    "            sum_error += (expected_value-sum(outputs))**2\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, dataset[i], l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "        if (sum_error_old > 0 and abs(sum_error_old-sum_error) < stop_rate): \n",
    "            print(\"O sum error old é %f, e o sum error atual é %f\" %(sum_error_old, sum_error))\n",
    "            print(\"SAI DO MEU TREINAMENTO, CONVERGIU!\")\n",
    "            break\n",
    "        sum_error_old = sum_error\n",
    "        \n",
    "# Treinamento com recorrência\n",
    "def train_network(network, dataset, l_rate, n_epoch, n_outputs, stop_rate, atrasos):\n",
    "    expected = list()\n",
    "    sum_error_old = 0.000\n",
    "    lista_inputs = list()\n",
    "    for i in range(len(dataset)-1):\n",
    "        expected.append(dataset[i+1][-1])\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for i in range (0, atrasos+1):\n",
    "            lista_inputs.append(0)\n",
    "        ## ACRESCENTA OS INPUTS A CADA ITERAÇÃO DE ATRASO\n",
    "        for i in range(0, atrasos):\n",
    "            lista_inputs.insert(1, dataset[i][0])\n",
    "            lista_inputs.pop(0)\n",
    "            inputs = lista_inputs\n",
    "            outputs = forward_propagate3(network, inputs)\n",
    "            expected_value = expected[i]\n",
    "            sum_error += (expected_value-sum(outputs))**2  \n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, dataset[i], l_rate) \n",
    "            lista_inputs.append(sum(outputs))\n",
    "            \n",
    "        ## APÓS OS ATRASOS FEITOS, COMEÇA A PROCESSAR O RESTO DA LISTA\n",
    "        for i in range(atrasos, len(dataset)-1):\n",
    "            lista_inputs.insert(1, dataset[i][0])\n",
    "            # retira a entrada anterior da lista de inputs\n",
    "            lista_inputs.pop(0)\n",
    "            inputs = lista_inputs\n",
    "            outputs = forward_propagate3(network, inputs)\n",
    "            expected_value = expected[i]\n",
    "            sum_error += (expected_value-sum(outputs))**2\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, dataset[i], l_rate)\n",
    "            lista_inputs.append(sum(outputs))\n",
    "            # retira o elemento atrasado mais velho da lista de inputs\n",
    "            lista_inputs.pop(1)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "        # repopula a lista\n",
    "        lista_inputs.clear()\n",
    "        if (sum_error_old > 0 and abs(sum_error_old-sum_error) < stop_rate): \n",
    "            return sum_error\n",
    "        sum_error_old = sum_error\n",
    "    return sum_error\n",
    "\n",
    "# Initialize a network - tradicional, com bias no output\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "# Initialize a network - tradicional, com atraso\n",
    "def initialize_network2(n_inputs, n_hidden, n_outputs, n_atrasos):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + n_atrasos + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "# Initialize a network - tradicional, com atraso, arquitetura com várias hidden layers\n",
    "def initialize_network3(n_inputs, n_hidden, n_layers, n_outputs, n_atrasos):\n",
    "    network = list()\n",
    "    for i in range(n_layers):\n",
    "        hidden_layer = [{'weights':[random() for i in range(n_inputs + n_atrasos + 1)]} for i in range(n_hidden)]\n",
    "        network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "# Acrescenta mais um valor na camada de entrada e a seu correspondente na camada oculta\n",
    "def acrescenta_input(network):\n",
    "    camada = network[0]\n",
    "    neuronio = camada[0]\n",
    "    network.reverse()\n",
    "    neuronio['weights'].append(random())\n",
    "    network.reverse()\n",
    "    return network\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return sum(outputs)\n",
    "\n",
    "#Soma os valores de uma saída e faz a reconversão Min-Max\n",
    "def calcula_saida(layer):\n",
    "    cont = 0\n",
    "    #return sum(layer)\n",
    "    for i in range(len(layer)):\n",
    "        neuron = layer[i]\n",
    "        cont += neuron['output']\n",
    "    #cont = transfer(cont)\n",
    "    #return (maprange((0,1),(-1,1),cont))\n",
    "    return cont\n",
    "    #return scaler.inverse_transform(cont)\n",
    "    \n",
    "# converte range de saída\n",
    "def maprange(a, b, s):\n",
    "    (a1, a2), (b1, b2) = a, b\n",
    "    return  b1 + ((s - a1) * (b2 - b1) / (a2 - a1))\n",
    "\n",
    "def testa_rede (treinamento, n_inputs, n_outputs, n_hidden, n_atrasos, n_epoch, stop_rate):\n",
    "    melhor = 100\n",
    "    hidden = 0\n",
    "    atrasos = 0\n",
    "    for i in range(1, n_hidden):\n",
    "        for j in range(0, n_atrasos):\n",
    "            rede = initialize_network2(n_inputs,n_hidden,n_outputs,n_atrasos)\n",
    "            x = train_network(rede, treinamento, l_rate, n_epoch, n_inputs, stop_rate, n_atrasos)\n",
    "            if (x < melhor):\n",
    "                melhor = x\n",
    "                hidden = n_hidden\n",
    "                atrasos = n_atrasos\n",
    "    print(\"A melhor rede foi a com %d hidden e %d atrasos, tendo um resultado de %f de erro\" %(hidden,atrasos,melhor))\n",
    "    \n",
    "    \n",
    "    \n",
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'serie2.csv'\n",
    "treinamento = load_csv(filename)\n",
    "#scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#dataset = scaler.fit_transform(treinamento)\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(treinamento)\n",
    "normalize_dataset(treinamento, minmax)\n",
    "\n",
    "# evaluate algorithm\n",
    "l_rate = 0.3\n",
    "n_epoch = 1000\n",
    "n_hidden = 10\n",
    "n_inputs = len(treinamento[0])\n",
    "n_outputs = 1\n",
    "stop_rate = 10e-5\n",
    "n_atrasos = 10\n",
    "n_layers = 2\n",
    "network = initialize_network2(n_inputs, n_hidden, n_outputs, n_atrasos)\n",
    "#network = initialize_network3(n_inputs, n_hidden, n_layers, n_outputs, n_atrasos)\n",
    "train_network(network, treinamento, l_rate, n_epoch, n_inputs, stop_rate, n_atrasos)\n",
    "#testa_rede(treinamento,n_inputs,n_outputs,n_hidden,n_atrasos,n_epoch, stop_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
